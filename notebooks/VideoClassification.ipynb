{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5daa40",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ddf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Classification libraries\n",
    "from openai import OpenAI, RateLimitError, APIError, Timeout, APIConnectionError\n",
    "\n",
    "# Retry libraries\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception_type\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe25a7e",
   "metadata": {},
   "source": [
    "# Classificator configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5d1be",
   "metadata": {},
   "source": [
    "## GPT configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI4_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OPENAI4_API_KEY not found in environment variables\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Classification Function\n",
    "@retry(\n",
    "    retry=retry_if_exception_type((RateLimitError, APIError, Timeout, APIConnectionError)),\n",
    "    wait=wait_random_exponential(min=5, max=60),\n",
    "    stop=stop_after_attempt(6)\n",
    ")\n",
    "def openAI_classificator(context, message, model=\"gpt-4.1-2025-04-14\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": context\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": message\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=10, \n",
    "            temperature=0.0,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content, response.usage.total_tokens\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        match = re.search(r\"Requested (\\d+)\\.\", str(e))\n",
    "        if match:\n",
    "            requested_tokens = int(match.group(1))\n",
    "        else:\n",
    "            requested_tokens = 0\n",
    "        \n",
    "        return np.nan, requested_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6b6b7",
   "metadata": {},
   "source": [
    "## Token limit config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f964295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max token per minute \n",
    "MAX_TOKENS_PER_MINUTE = 450000 # Rate limit tier 2 -> 450k tokens/minute in model gpt-4.1-2025-04-14\n",
    "\n",
    "# Save (timestamp, tokens) of each request in a deque\n",
    "token_window = deque()\n",
    "\n",
    "def clean_token_window():\n",
    "    # Delete the values older than 60 seconds\n",
    "    current_time = time.time()\n",
    "    while token_window and (current_time - token_window[0][0]) > 60:\n",
    "        token_window.popleft()\n",
    "\n",
    "def wait_if_needed(new_tokens):\n",
    "    clean_token_window()\n",
    "    current_tokens = sum(tokens for _, tokens in token_window)\n",
    "\n",
    "    if current_tokens + new_tokens > MAX_TOKENS_PER_MINUTE:\n",
    "        excess = (current_tokens + new_tokens) - MAX_TOKENS_PER_MINUTE\n",
    "        print(f\"[WAIT] amount of tokens exceded ({excess}). Waiting...\")\n",
    "\n",
    "        # Calculate the time to wait based on the excess tokens\n",
    "        time_to_wait = 60 - (time.time() - token_window[0][0])\n",
    "        time.sleep(time_to_wait)\n",
    "        clean_token_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d649991",
   "metadata": {},
   "source": [
    "# Context and subject definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Your context goes here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738574ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"Your subject goes here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a5657",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef515fe",
   "metadata": {},
   "source": [
    "## Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d31bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileV = \"your_file.csv\"\n",
    "dfV = pd.read_csv(fileV, dtype={\"classification\": str, \"polarity\":str})\n",
    "dfV.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ccad5e",
   "metadata": {},
   "source": [
    "## Prompt definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_prompt(context,subject):\n",
    " return f\"\"\" \n",
    "    Contexto: {context}\n",
    "    \n",
    "    Instrucción: Clasifica el siguiente resumen en dos escalas, la primera es la de Likert y la segunda es la de polaridad. Para la escala Likert\n",
    "    ten en cuenta las siguientes opciones: 1:'Completamente en desacuerdo', 2:'En desacuerdo', 3:'Ni de acuerdo ni en desacuerdo', 4:'De acuerdo', 5:'Completamente de acuerdo'.\n",
    "    Para la escala de polaridad ten en cuenta las siguientes opciones: 1:'Negativo', 2:'Neutro', 3:'Positivo'. En ambos casos el resumen se debe\n",
    "    clasificar en relación con la siguiente afirmación o pregunta: \\\"{subject}\\\".\n",
    "\n",
    "    Solo responde con una de las etiquetas mencionadas (solo el número) sin ningún texto adicional. Por ejemplo \"1,2\" o \"3,1\" o \"5,3\", donde el primer numero sea\n",
    "    la escala Likert y el segundo la escala de polaridad.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138c7f9",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be59ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = 0\n",
    "\n",
    "for i,row in dfV.iterrows():\n",
    "    if pd.notnull(dfV.at[i, 'classification']):\n",
    "        continue\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing row {i}/{len(dfV)}\")\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        # Save the dataframe every 20 rows\n",
    "        dfV.to_csv(fileV, index=False)\n",
    "\n",
    "    resumen = row['summary_Sumy']\n",
    "    prompt = video_prompt(context,subject)\n",
    "\n",
    "    try:\n",
    "        # Check if is needed to wait\n",
    "        estimated_tokens = 1500  # This value can be adjusted (an estimated in this case is 650, but we add 150 for safety)\n",
    "        wait_if_needed(estimated_tokens)\n",
    "\n",
    "        response = openAI_classificator(prompt, resumen)\n",
    "        classification, polarity = response[0].split(\",\")\n",
    "        used_tokens = response[1]\n",
    "\n",
    "        # Save the tokens used\n",
    "        dfV.at[i, 'classification'] = classification\n",
    "        dfV.at[i, 'polarity'] = polarity\n",
    "\n",
    "        # Update the token window\n",
    "        token_window.append((time.time(), used_tokens))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la fila {i}: {e}\")\n",
    "        dfV.at[i, 'classification'] = 'error'\n",
    "        dfV.at[i, 'polarity'] = 'error'\n",
    "\n",
    "    if i == len(dfV):\n",
    "            # Save the dataframe at the end\n",
    "            dfV.to_csv(fileV, index=False)\n",
    "\n",
    "dfV.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
